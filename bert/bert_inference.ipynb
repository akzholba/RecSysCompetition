{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert4rec import Bert4Rec, Bert4RecModel\n",
    "best_model = Bert4Rec.load_from_checkpoint('./epoch=92-step=1116.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Replay ALS model support only spark 3.1-3.4 versions! Replay will use 'https://repo1.maven.org/maven2/io/github/sb-ai-lab/replay_2.12/3.1.3/replay_2.12-3.1.3.jar' in 'spark.jars' property.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/25 22:23:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/10/25 22:23:58 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from prediction_callbacks import (\n",
    "    SparkPredictionCallback,\n",
    "    PandasPredictionCallback,\n",
    "    TorchPredictionCallback,\n",
    "    QueryEmbeddingsPredictionCallback,\n",
    ")\n",
    "from schema import (\n",
    "    FeatureHint,\n",
    "    FeatureInfo,\n",
    "    FeatureSchema,\n",
    "    FeatureSource,\n",
    "    FeatureType,\n",
    ")\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from sequence_tokenizer import SequenceTokenizer\n",
    "from postprocessors import RemoveSeenItems\n",
    "from last_n_splitter import LastNSplitter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from data import Dataset, get_spark_session\n",
    "from schema import (\n",
    "    TensorFeatureSource,\n",
    "    TensorSchema,\n",
    "    TensorFeatureInfo)\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import (\n",
    "    Bert4RecPredictionDataset,\n",
    ")\n",
    "\n",
    "MAX_SEQ_LEN = 100\n",
    "BATCH_SIZE = 512\n",
    "NUM_WORKERS = 4\n",
    "# --------------------\n",
    "spark_session = get_spark_session()\n",
    "le = LabelEncoder()\n",
    "\n",
    "interactions = pd.read_csv('../data/events.csv')\n",
    "interactions[interactions['user_id']==0]\n",
    "\n",
    "item_features = pd.read_csv('../data/item_features.csv')\n",
    "\n",
    "user_features = pd.read_csv('../data/user_features.csv')\n",
    "user_features['gender'] = le.fit_transform(user_features['gender'])\n",
    "\n",
    "splitter = LastNSplitter(\n",
    "    N=1,\n",
    "    divide_column=\"user_id\",\n",
    "    query_column=\"user_id\",\n",
    "    strategy=\"interactions\",\n",
    ")\n",
    "\n",
    "raw_test_events, raw_test_gt = splitter.split(interactions)\n",
    "raw_validation_events, raw_validation_gt = splitter.split(raw_test_events)\n",
    "raw_train_events = raw_validation_events\n",
    "def prepare_feature_schema(is_ground_truth: bool) -> FeatureSchema:\n",
    "    base_features = FeatureSchema(\n",
    "        [\n",
    "            FeatureInfo(\n",
    "                column=\"user_id\",\n",
    "                feature_hint=FeatureHint.QUERY_ID,\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=\"item_id\",\n",
    "                feature_hint=FeatureHint.ITEM_ID,\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    if is_ground_truth:\n",
    "        return base_features\n",
    "\n",
    "    all_features = base_features + FeatureSchema(\n",
    "        [\n",
    "            FeatureInfo(\n",
    "                column=\"timestamp\",\n",
    "                feature_type=FeatureType.NUMERICAL,\n",
    "                feature_hint=FeatureHint.TIMESTAMP,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return all_features\n",
    "train_dataset = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=False),\n",
    "    interactions=raw_train_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "\n",
    "ITEM_FEATURE_NAME = \"item_id_seq\"\n",
    "\n",
    "tensor_schema = TensorSchema(\n",
    "    TensorFeatureInfo(\n",
    "        name=ITEM_FEATURE_NAME,\n",
    "        is_seq=True,\n",
    "        feature_type=FeatureType.CATEGORICAL,\n",
    "        feature_sources=[TensorFeatureSource(FeatureSource.INTERACTIONS, train_dataset.feature_schema.item_id_column)],\n",
    "        feature_hint=FeatureHint.ITEM_ID,\n",
    "        embedding_dim=300,\n",
    "    )\n",
    ")\n",
    "tokenizer = SequenceTokenizer(tensor_schema, allow_collect_to_master=True)\n",
    "tokenizer.fit(train_dataset)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "validation_dataset = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=False),\n",
    "    interactions=raw_validation_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "validation_gt = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=True),\n",
    "    interactions=raw_validation_gt,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "test_dataset = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=False),\n",
    "    interactions=raw_test_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "test_gt = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=True),\n",
    "    interactions=raw_test_gt,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "test_query_ids = test_gt.query_ids\n",
    "test_query_ids_np = tokenizer.query_id_encoder.transform(test_query_ids)[\"user_id\"].values\n",
    "sequential_test_dataset = tokenizer.transform(test_dataset).filter_by_query_id(test_query_ids_np)\n",
    "TOPK = [1, 10, 20, 100]\n",
    "\n",
    "postprocessors = [RemoveSeenItems(sequential_test_dataset)]\n",
    "\n",
    "spark_prediction_callback = SparkPredictionCallback(\n",
    "    spark_session=spark_session,\n",
    "    top_k=max(TOPK),\n",
    "    query_column=\"user_id\",\n",
    "    item_column=\"item_id\",\n",
    "    rating_column=\"score\",\n",
    "    postprocessors=postprocessors,\n",
    ")\n",
    "\n",
    "pandas_prediction_callback = PandasPredictionCallback(\n",
    "    top_k=max(TOPK),\n",
    "    query_column=\"user_id\",\n",
    "    item_column=\"item_id\",\n",
    "    rating_column=\"score\",\n",
    "    postprocessors=postprocessors,\n",
    ")\n",
    "\n",
    "torch_prediction_callback = TorchPredictionCallback(\n",
    "    top_k=max(TOPK),\n",
    "    postprocessors=postprocessors,\n",
    ")\n",
    "prediction_dataloader = DataLoader(\n",
    "    dataset=Bert4RecPredictionDataset(\n",
    "        sequential_test_dataset,\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\".logs/test\", name=\"Bert4Rec_example\")\n",
    "\n",
    "query_embeddings_callback = QueryEmbeddingsPredictionCallback()\n",
    "csv_logger = CSVLogger(save_dir=\".logs/train\", name=\"Bert4Rec_example\")\n",
    "trainer = L.Trainer(\n",
    "    callbacks=[\n",
    "        spark_prediction_callback,\n",
    "        pandas_prediction_callback,\n",
    "        torch_prediction_callback,\n",
    "        query_embeddings_callback,\n",
    "    ],\n",
    "    logger=csv_logger,\n",
    "    inference_mode=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akzholba/Documents/RecSys/RecSysCompetition/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'predict_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 12/12 [01:24<00:00,  0.14it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/25 22:25:50 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/10/25 22:25:50 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/10/25 22:25:50 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    }
   ],
   "source": [
    "#trainer goes brrrrrr\n",
    "trainer.predict(best_model, dataloaders=prediction_dataloader, return_predictions=False)\n",
    "\n",
    "spark_res = spark_prediction_callback.get_result()\n",
    "pandas_res = pandas_prediction_callback.get_result()\n",
    "torch_user_ids, torch_item_ids, torch_scores = torch_prediction_callback.get_result()\n",
    "user_embeddings = query_embeddings_callback.get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Replay ALS model support only spark 3.1-3.4 versions! Replay will use 'https://repo1.maven.org/maven2/io/github/sb-ai-lab/replay_2.12/3.1.3/replay_2.12-3.1.3.jar' in 'spark.jars' property.\n",
      "WARNING:root:Replay ALS model support only spark 3.1-3.4 versions! Replay will use 'https://repo1.maven.org/maven2/io/github/sb-ai-lab/replay_2.12/3.1.3/replay_2.12-3.1.3.jar' in 'spark.jars' property.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+-------+\n",
      "|            score|user_id|item_id|\n",
      "+-----------------+-------+-------+\n",
      "|9.312718391418457|      0|   3421|\n",
      "|8.847206115722656|      0|   1422|\n",
      "|8.750907897949219|      0|    434|\n",
      "|8.747825622558594|      0|    708|\n",
      "| 8.62032413482666|      0|   3025|\n",
      "|8.587139129638672|      0|   2543|\n",
      "|8.581725120544434|      0|   1461|\n",
      "|8.497233390808105|      0|   2003|\n",
      "| 8.42972469329834|      0|   1332|\n",
      "|8.354842185974121|      0|   1287|\n",
      "|8.291471481323242|      0|    566|\n",
      "|8.272330284118652|      0|    827|\n",
      "|8.268226623535156|      0|   2138|\n",
      "|8.233556747436523|      0|   3460|\n",
      "|8.218438148498535|      0|   2108|\n",
      "|8.189332008361816|      0|    980|\n",
      "| 8.14272403717041|      0|   2251|\n",
      "|8.138208389282227|      0|   1250|\n",
      "|7.928656101226807|      0|   1951|\n",
      "|7.888411045074463|      0|   1128|\n",
      "+-----------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recommendations = tokenizer.query_and_item_id_encoder.inverse_transform(spark_res)\n",
    "recommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akzholba/Documents/RecSys/RecSysCompetition/.venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Преобразуем Spark DataFrame в Pandas DataFrame\n",
    "pandas_df2 = recommendations.toPandas()\n",
    "\n",
    "# Сохраняем результаты в CSV-файл\n",
    "pandas_df2.to_csv('recommendations.csv', index=False)\n",
    "\n",
    "def get_top_n(user_item_ratings, model_name, n=100):\n",
    "    '''Функция возвращает топ-n фильмов для каждого пользователя'''\n",
    "\n",
    "    # Сортируем данные по убыванию предсказанной оценки\n",
    "    top_n = user_item_ratings.sort_values(model_name, ascending=False)\n",
    "\n",
    "    # Оставляем только первые n строк для каждого пользователя\n",
    "    top_n = top_n.groupby('user_id').head(n).reset_index(drop=True)\n",
    "\n",
    "    return top_n\n",
    "top_10_films = get_top_n(pandas_df2, 'score', n=10)[['user_id', 'item_id']]\n",
    "\n",
    "# Экспортируем результат в CSV-файл\n",
    "top_10_films.to_csv('top_10_films.csv', index=False)\n",
    "def format_for_submission(df):\n",
    "    # Группируем строки по user_id и соединяем item_id через пробел\n",
    "    submission = (\n",
    "        df\n",
    "        .groupby('user_id')['item_id']\n",
    "        .apply(lambda x: ' '.join(x.astype(str)))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return submission\n",
    "submission = format_for_submission(top_10_films)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10 = 0.0208\n"
     ]
    }
   ],
   "source": [
    "from utils_bert.recall_at_k import *\n",
    "submission_file_path = 'submission.csv'\n",
    "real_interactions_file_path = '../data/dataset_to_recall.csv'\n",
    "\n",
    "dataset_for_recall = pd.read_csv(real_interactions_file_path)\n",
    "submission_df = pd.read_csv(submission_file_path)\n",
    "\n",
    "submission_df['item_id'] = submission_df['item_id'].apply(lambda x: x.split())\n",
    "dataset_for_recall['last_10_interactions'] = dataset_for_recall['last_10_interactions'].apply(lambda x: x.split())\n",
    "\n",
    "submission_df['y_real']  = dataset_for_recall['last_10_interactions']\n",
    "\n",
    "result = recall_at_k_overall(submission_df, actual_col='y_real', predicted_col='item_id')\n",
    "print(f\"Recall@10 = {result:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
